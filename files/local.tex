\section{Localization}\label{sec:local}

The more general case for
notions described below can be found in
\cite{LNep}

\begin{rdef}{Definition}
Let $A$ be a locally compact metric space. 
We say that a point  $p\in A$
is  \emph{ $\epsilon$-inner point} if
the closed ball $\bar B(x,2\cdot\epsilon)$ is compact.

\end{rdef}

\begin{rdef}{Definition}
We say that
 a locally compact inner metric space $A$ of finite Hausdorff dimension
is an \emph{ Alexandrov region} if
any point has a neighborhood where Alexandrov
comparison for curvature $\ge -1$ holds.
We call a comparison 
radius (and denote it $r_c(p)$) a maximal number $r$ such that
$p$ is $r$-inner point and 
 Alexandrov
comparison for curvature $\ge -1$ holds in
$B(x,r)$.

%We say that $U\subset A$ is an \emph{ Alexandrov domain} if $U$ is a strongly inner domain.

\end{rdef}
It is possible to show the following uniform estimate for
comparison radius and convex neighborhood (for the proof, see \cite{LNep}).

\begin{thm}{Proposition}
Let $A$ be an $m$-dimensional Alexandrov region
and $x$ be a $\varepsilon$-inner
point. Then 
 $r_c(x)\ge 0.01\varepsilon$.
Moreover,  for some constant $c_{conv}(m)$,
the ball $B(x, c_{conv}(m)r(p))$ is a subset of some
convex
neighborhood of $x$ in $A$.

\end{thm}

Note that a compact convex subset in an Alexandrov region is an Alexandrov space.
So the above proposition
makes possible 
to apply most of the arguments and constructions for Alexandorov spaces to
spaces
  with local structure of Alexandrov space. 
  Moreover, in the case when Alexandrov region
  is a Riemannian 
manifold (possibly noncomplete) it is possible
to take the doubling of convex neighborhood from the Proposition
and smooth it with almost the same lower curvature bound.
This allows us to apply the main result from
 \cite{petrunin-SC}, where the complete manifold can be replaced by a convex domain  in a possibly open manifold. 

Further, let us define a local version of smoothing.
Let us denote by
$\M_{\ge -1}^m$ a class of $m$-dimensional Riemannian 
manifolds without boundary, but possibly non-complete, with sectional curvature bounded
from below by $-1$.

\begin{rdef}{Definition}
Let 
$M_n\in\M_{\ge -1}^m$ (with corresponding intrinsic metric)
converges in Gromov--Hausdorff sense to some metric space $A$ via
approximation.
Suppose that $M_n\ni x_n\to x\in A$
and $r_c(x_n)\ge c_0>0$. Let $c_{conv}(m)c_0 > R>0$ and set
$U_n=B(x_n,R)_{M_n}$.
Then we say that $U_n$ is a local smoothing of $U=B(x,R)_A$ (briefly, $U_n\smooths{} U$).
\end{rdef}

It is straightforward to redefine test functions and weak convergence for local smoothings.
Using this language we can make local version for each statement in this paper, the proofs go without changes.
A couple of times we had to use these local versions, so in a perfect world we had to rewire the whole paper using this language.
However, this is not a principle moment,
so we decided to keep paper more readable at the cost of being not fully rigorous. 

As a result, we get the following local version of the main theorem \ref{main}.
 
\begin{thm}{Local version of the main theorem}\label{mainloc}
Let   
$M_n\in\M_{\ge -1}^m$,
$M_n\GHto A$, $U_n\subset M_m$,
  $U\subset A$, and $U_n\smooths{} U$.
  
Denote by $\qm_n$ the dual measure-valued curvature tensor on $U_n$.
Then there is a measure-valued tensor $\qm$ on $U$ such that $\qm_n\rightharpoonup \qm$.
\end{thm}

\section{A more conceptual definition of convergence}

Here we will describe a more natural definition of convergence that is equivalent to our definition.
It is given only for aesthetic purposes, we do not see its application so far.
By that reason, we give definitions and do not prove equivalence.

\parbf{Convergence of vectors.}
Let $A$ be an Alexandrov space, we denote by $\T A$ the set of all tangent vectors at all points.
So far $\T A$ is a disjoint union of all tangent cones;
let us define convergence on it.

Recall that gradient exponent $\gexp\: \T A\to A$ is defined in \cite{AKP}.
Given a vector $V\in \T A$, it defines its gradient curve $\gamma_{V}\:t\mapsto \gexp (t\cdot V)$.
We say that a sequence of vectors $V_n\in \T A$ converges to $V\in \T A$ if $\gamma_{V_n}$ converges to $\gamma_V$ pointwise.
Since the gradient curve $\gamma_V$ is $|V|$-Lipschitz, we get that for any bounded sequence of vectors with base points in a bounded set have a converging subsequence of $\gamma_{V_n}$.
Further, the pointwise limit of such curves is a gradient curve as well.
Therefore, any bounded sequence of tangent vectors with base points in a bounded set has a converging sequence.

A direct analog holds for sequences of Alexandrov spaces $A_n$ that converge to $A$.
That is, if $V_n\in \T A_n$ is a bounded sequence of tangent vectors with bounded set of base points, then it has a subsequence that converges to some vector $V\in \T A$.

Note that 
\[|V|\le \lim |V_n|\]
and the inequality might be strict.




\parbf{$\bm{C^1}$-delta smoothfunctions and their convergence.}
Given a function $f\:A\to \RR$ and a vector $V\in \T A$, set
\[Vf=(f\circ\gamma_V(t))'|_{t=0}.\]
Note that $Vf$ is defined for all DC-functions.

Two vectors $V,W\in \T_pA$ will be called $\delta$-opposite if
$1-\delta< |V|\le 1$,
$1-\delta< |W|\le 1$,
and $|\langle X,V\rangle +\langle X,W\rangle|<\delta$ for any unit vector $X\in T_p A$.
We say that $V,W\in \T_pA$ are opposite if they are $\delta$-opposite for any $\delta>0$;
in this case they are both unit vectors and make angle $\pi$ to each other.  

A Lipschitz function $f\:A\to \RR$ will be called \emph{$C^1$-delta smooth} if $Vf$ is defined for any unit vector $V$ and it satisfies the following continuity property:
for any $\eps>0$ there is $\delta>0$ such that if unit vectors $V_n$ converge to $V$ and $V$ has a $\delta$-opposite vector, then 
\[|Vf-\lim V_nf|<\eps.\eqlbl{eq:Vf}\]

Similarly, we can define $C^1$-delta converging sequence. Namely, suppose $A_n\GHto A$, then a sequence of $C^1$-delta smooth functions $f_n\:A_n\to\RR$ \emph{$C^1$-delta converges} to $f\:A\to \RR$ if for any $\eps>0$ there is $\delta>0$ such that any sequence of unit vectors $V_n\in \T A_n$ that converges to a vector $V\in \T A$ that has a $\delta$-opposite vector \ref{eq:Vf} holds. 

\parbf{Weak convergence of tensor fields}
It is easy to see that test-convergence implies $C^1$-delta convergence.
The latter notion seems to be more natural.
If we replace test convergence
by $C^1$-delta convergence
in the definition of the weak convergence of tensor fields and
formulate our Main theorem using this definition we get formally stronger
statement but in fact our proof 
still works for this.

For the part with DC-calculus and 
Key lemma we still use test sequences
(note that $C^1$-delta smooth functions are not DC in general).
Then we need only
the following property. 
For any test sequence $f_n$
and any $C^1$-delta converging
sequence $g_n$ the function
$\langle \nabla f_n , \nabla g_n
\rangle$ delta-converges.


%It is possible to redefine weak convergence using $C^1$-delta convergence instead of test-convergence.

The main part of the proof of this statement is analogous to differentiability of a function with continuous partial derivatives. 
